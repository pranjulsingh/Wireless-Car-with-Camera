#!/usr/bin/env python
# coding: utf-8

import pandas as pd

# Reading file into pandas dataframe
dataset = pd.read_csv('bbc-text.csv')

# Print shape of dataset
print(dataset.shape)

# Print top 5 rows of dataset
dataset.head()

# Removing extra spaces and converting all to lowercase
dataset['text'] = dataset['text'].apply(lambda x : x.strip(' '))
dataset['text'] = dataset['text'].apply(lambda a: a.lower())
print('Unique values in action: ',len(set(dataset['text'])))

# Checking for null values
print(dataset.text.isna().sum())

# Checkning for categories in 
print(dataset.category.value_counts())

# Tokenizing each sentence into the list of words
import nltk
from nltk.tokenize import RegexpTokenizer
tokenizer = RegexpTokenizer(r'\w+')
dataset.text = dataset.text.apply(lambda a: tokenizer.tokenize(a))
dataset.head()

# Removing stop words like in, on, to etc. from the list of tokens
from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
dataset.text = dataset.text.apply(lambda a: [w for w in a if w not in stop_words])
dataset.head()

# Converting each word in to its base form
from nltk.stem import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()
dataset.text = dataset.text.apply(lambda a: [lemmatizer.lemmatize(w, pos='v') for w in a])

# Printing top 5 rows of processed dataset
print(dataset.head())

# Saving processed dataset
dataset.to_csv("processed_data.csv")

# Converting the text column into one-hot-encoded values
# Each column representing single unique word
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()
X = mlb.fit_transform(dataset.text)
print(X.shape)

# Encoding categorical values into numbers
from sklearn import preprocessing
le = preprocessing.LabelEncoder()
y = le.fit_transform(dataset.category)
print(y.shape)

# Splitting dataset into train and test
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# Creating deep neural network classifier
import tensorflow as tf
feature_columns = [tf.feature_column.numeric_column("x", shape=[X_train.shape[1]])]
classifier = tf.estimator.DNNClassifier(
 feature_columns=feature_columns,
 hidden_units=[128, 64],
 optimizer=tf.train.AdamOptimizer(1e-4),
 n_classes=5,
 dropout=0.1,
)

# Preparing training config and training the model
training_set = tf.estimator.inputs.numpy_input_fn(
 x={"x": X_train},
 y=y_train.astype(int),
 num_epochs=None,
 batch_size=50,
 shuffle=True
)

classifier.train(input_fn=training_set, steps=100)

test_input_fn = tf.estimator.inputs.numpy_input_fn(
 x={"x": X_test},
 y=y_test.astype(int),
 num_epochs=1,
 shuffle=False
)

accuracy = classifier.evaluate(input_fn=test_input_fn)["accuracy"]
print("Accuracy on test set: {0:f}%\n".format(accuracy*100))
